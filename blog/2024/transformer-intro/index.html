<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> An Intro to Transformer-based Language Models (LMs) | Kowndinya Renduchintala </title> <meta name="author" content="Hari Sri Venkata Naga Sai Kowndinya Renduchintala"> <meta name="description" content="Kowndinya's Personal Website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="kowndinya-renduchintala"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot-thinking.avif?2fa9026c07e9fd62db704365d6a647b6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "An Intro to Transformer-based Language Models (LMs)",
            "description": "",
            "published": "December 15, 2024",
            "authors": [
              
              {
                "author": "H S V N S Kowndinya Renduchintala",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "MDSR, Adobe",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Kowndinya Renduchintala </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>An Intro to Transformer-based Language Models (LMs)</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <p><strong>Note</strong>: This is just an article (not a paper) explaining the basics of transformer-based LMs.</p> <h2 id="what-is-a-transformer-based-lm">What is a Transformer-based LM?</h2> <p>Transformers are a class of artificial neural networks that are used to model sequences e.g., natural language text. In this article, we primarily focus on GPT-style transformers. At a fundamental level, given a piece of natural language text, these transformer-based LMs generate a probability distribution over the vocabulary. Upon repeatedly sampling from these distributions, new text can be generated. Hence the name <em>Generative</em> Pretrained Transformers(GPT).</p> <h2 id="how-are-transformers-pre-trained">How are transformers pre-trained?</h2> <p>Language Modeling is typically framed as a problem of <strong>unsupervised distribution estimation</strong> from a set of training examples \(\mathcal{T}=(x_1, x_2, \dots, x_N)\), where each \(x_i\) is composed of variable length sequences of tokens \((\mathbf{t_{i}^{(1)}}, \mathbf{t_{i}^{(2)}}, \dots ,\mathbf{t_{i}^{(N_i)}})\). Note that each \(\mathbf{t_i^{(k)}}\) is a \(\lvert\mathcal{V}\rvert\)-dimensional one-hot vector where \(\mathcal{V}\) is the vocabulary of the language being modeled.</p> <p>The joint probabilities over tokens can be written as the product of conditional probabilities as follows:</p> \[P(x_i)=\prod_{k=1}^{N_i}P\left(\mathbf{t_i^{(k)}} \mid\mathbf{t_i^{(1)}}, \mathbf{t_i^{(2)}}, \dots, \mathbf{t_i^{(k-1)}}\right)\] <p>Language Models (consisting of parameters \(\mathbf{\Theta}=\{\theta_i\}_{i=1}^{P}\) where \(P\) can be of the order of few billions), are trained to maximize the (log-)likelihood of text in their training set \(\mathcal{T}\) i.e., they try to estimate \(\mathbf{\Theta^*}\) where</p> \[\mathbf{\Theta^*}= \underset{\mathbf{\Theta}}{\mathrm{argmax}} \log \prod_{i=1}^{N}P(x_i;\mathbf{\Theta}) = \underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N}\log P(x_i;\mathbf{\Theta})\] \[=\underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N} \sum_{k=1}^{N_i} \log P\left(\mathbf{t_i^{(k)}} \mid\mathbf{t_i^{(1)}}, \mathbf{t_i^{(2)}}, \dots, \mathbf{t_i^{(k-1)}};\mathbf{\Theta}\right)\] \[\mathbf{\Theta^*}=\underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N} \sum_{k=1}^{N_i} \mathbf{t_i^{(k)}} . \log \mathbf{y_i^{(k)}}(\mathbf{t_i^{(1)}},\mathbf{t_i^{(2)}},\dots,\mathbf{t_i^{(k-1)}},\mathbf{\Theta})\] <p>Here, \(\mathbf{y_i^{(k)}}\left(\mathbf{t_i^{(1)}},\mathbf{t_i^{(2)}},\dots,\mathbf{t_i^{(k-1)}},\mathbf{\Theta}\right) \in \mathbb{R}^{\lvert\mathcal{V}\rvert}\) is the vector of probabilities that are predicted by the model for the token \(\mathbf{t_i^{(k)}}\), given previous tokens in the input and the model parameters \(\mathbf{\Theta}\). In the above equation, \(.\) represents inner product and logarithm is element-wise. The negative of the above objective is also called as Cross-Entropy Loss.</p> <p><strong>Remark</strong> Note that if you give a transformer \(N_i\) tokens, it predicts the next token for <strong><em>each</em></strong> of the \(N_i\) prefixes i.e., it produces \(N_i\) predictions. But why? Turns out it is easier to make one that does this. Moreover this also makes training more efficient, because you get \(N_i\) bits of feedback rather than just one.</p> <p><strong>Remark</strong> Also note that we make the transformer have <strong><em>causal attention</em></strong> (a.k.a. autoregressive) i.e., it can move information only forwards in the sequence. The prediction of what comes after \(K\) tokens is only a function of the first \(K\) tokens.</p> <p>The key takeaway is that transformers are <em>sequence modeling engines</em>. They do the same processing in parallel at each sequence position, can move information between positions using attention, and conceptually can take a sequence of arbitrary length (not actually true as we see later)</p> <p><strong>Inputs to a transformer</strong> The inputs to a transformer are a sequence of tokens. Each token is a \(\lvert\mathcal{V}\rvert\)-dimensional one-hot encoded vector.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> Â© Copyright 2025 Hari Sri Venkata Naga Sai Kowndinya Renduchintala. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>