<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons | Kowndinya Renduchintala </title> <meta name="author" content="Hari Sri Venkata Naga Sai Kowndinya Renduchintala"> <meta name="description" content="Kowndinya's Personal Website. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="kowndinya-renduchintala"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/robot-thinking.avif?2fa9026c07e9fd62db704365d6a647b6"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://kowndinya-renduchintala.github.io/blog/2024/towards-understanding-mtl/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> <style type="text/css">.fake-img{background:#bbb;border:1px solid rgba(0,0,0,0.1);box-shadow:0 0 4px rgba(0,0,0,0.1);margin-bottom:12px}.fake-img p{font-family:monospace;color:white;text-align:left;margin:12px 0;text-align:center;font-size:16px}</style> </head> <body> <d-front-matter> <script async type="text/json">
      {
            "title": "Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons",
            "description": "",
            "published": "December 24, 2024",
            "authors": [
              
              {
                "author": "H S V N S Kowndinya Renduchintala",
                "authorURL": "",
                "affiliations": [
                  {
                    "name": "MDSR, Adobe",
                    "url": ""
                  }
                ]
              }
              
            ],
            "katex": {
              "delimiters": [
                {
                  "left": "$",
                  "right": "$",
                  "display": false
                },
                {
                  "left": "$$",
                  "right": "$$",
                  "display": true
                }
              ]
            }
          }
    </script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Kowndinya Renduchintala </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/index.html">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">Repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/gallery/">Gallery </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons</h1> <p></p> </d-title> <d-byline></d-byline> <d-article> <h2 id="introduction">Introduction</h2> <p>Instruction Tuning involves fine-tuning a language model on a collection of instruction-formatted multi-task datasets, with the goal of enabling the language model to generalize to unseen tasks. This paper investigates the inner workings of multi-task learning (from the perspective of neurons) in LMs, which still remains an open question. Specifically, the authors investigate the following three research questions:</p> <h3 id="research-questions">Research Questions</h3> <ol> <li>Do tasks-specific neurons exist in LMs, from a broad perpspective?</li> <li>If they exist, can they facilitate the understanding of the multi-task learning mechanisms in LMs?</li> <li>Can we improve LMs by exploring such neurons?</li> </ol> <p>On a very high level, this paper (empirically) detects task-sensitive neurons in LMs via gradient attribution on task-specific data and derives insights into generalization across tasks with the detected task-specific neurons. Further, a <strong>N</strong>euron-level <strong>C</strong>ontinuous learning <strong>F</strong>ine-<strong>T</strong>uning (NCFT) method is proposed for mitigating catastrophic forgetting.</p> <h2 id="methodology">Methodology</h2> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/figure-1-480.webp 480w,/assets/img/towards-understanding-mtl/figure-1-800.webp 800w,/assets/img/towards-understanding-mtl/figure-1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/towards-understanding-mtl/figure-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h3 id="identifying-task-specific-neurons">Identifying Task-Specific Neurons</h3> <p><strong>What is a neuron here?</strong> An autoregressive transformer-based LM (such as GPT-2) consists of an embedding layer, multiple residual blocks and an unembedding layer. Each residual block consists of a multi-head self-attention (MHA) module and a feed-forward network (FFN). The authors only focus on FFN citing that these have been demonstrated to store a large amount of parametric knowledge.</p> <p>The FFN module at layer \(i\) can be formulated as</p> \[\mathbf{h}^i=f(\mathbf{\tilde{h}^i}\mathbf{W}_1^i).\mathbf{W}_2^{i}\] <p>where \(\mathbf{\tilde{h}}^i\in\mathbb{R}^d\) denotes the output of the MHA module at layer \(i\), which is also the input of the current FFN module. \(\mathbf{h}^i\) denotes the output of the current FFN module. \(\mathbf{W}_1^i\in\mathbb{R}^{d\times d_{ff}}\) and \(\mathbf{W}_2^i\in\mathbb{R}^{d_{ff}\times d}\) are the weights of the FFN module at layer \(i\) and \(f(\cdot)\) denotes the activation function. A neuron is defined as a column in \(\mathbf{W}_1^i\) or \(\mathbf{W}_2^i\), which is a linear transformation of the input to the FFN module.</p> <p>Inspired from importance-based neuron fine-tuning studies and neuronal interpretability, the authors employ gradient-attribution to quantify each neuron’s relevance score for a given task. A relevance score \(\mathcal{R}_j^i\) is first defined of the \(j\)-th neuron in the \(i\)-th layer to a certain task:</p> \[\mathcal{R}_i^j=\left|\Delta \mathcal{L}(\mathbf{\omega}_j^i)\right|\] <p>where \(\mathbf{\omega}_j^i\) is the output of the \(j\)-th neuron in the \(i\)-th layer, and \(\Delta \mathcal{L}(\mathbf{\omega}_j^i)\) is the change in loss between setting \(\mathbf{\omega}_j^i\) to zero and the original value.</p> <p>Taylor Expansion can be used to approximate the change in loss when removing a particular neuron. Let \(\mathbf{\omega}^i\) be the output of the \(i\)-th layer and \(\Omega\) represent the set of other neurons. Assuming independence of each neuron in the model, the change of loss when removing the \(j\)-th neuron in layer \(i\) can be represented as:</p> \[\left| \Delta \mathcal{L}(\mathbf{\omega}_j^i) \right| = \left| \mathcal{L}(\Omega, \mathbf{\omega}_j^i=0) - \mathcal{L}(\Omega, \mathbf{\omega}_j^i) \right|\] <p>where \(\mathcal{L}(\Omega, \mathbf{\omega}_j^i=0)\) is the loss when the \(j\)-th neuron is pruned and \(\mathcal{L}(\Omega, \mathbf{\omega}_j^i)\) is loss if it is not pruned. The first-order Taylor series approximation for the function \(\mathcal{L}(\Omega, \mathbf{\omega}_j^i)\) at \(\mathbf{\omega}_j^i=0\) is:</p> \[\mathcal{L}(\Omega, \mathbf{\omega}_j^i) \approx \mathcal{L}(\Omega, \mathbf{\omega}_j^i=0) + \frac{\partial\mathcal{L}(\Omega, \mathbf{\omega}_j^i)}{\partial\mathbf{\omega}_j^i} \cdot \mathbf{\omega}_j^i\] <p>Hence, the relevance score can be approximated as:</p> \[\mathcal{R}_j^i \approx \left| \frac{\partial\mathcal{L}(\Omega, \mathbf{\omega}_j^i)}{\partial\mathbf{\omega}_j^i} \cdot \mathbf{\omega}_j^i \right|\] <p>Neurons with top \(k\%\) relevance scores are considered as task-specific neurons, where \(k\) is a predefined hyperparameter.</p> <h3 id="understanding-multi-task-learning-in-lms-by-analyzing-task-specific-neurons">Understanding Multi-Task Learning in LMs by analyzing task-specific neurons</h3> <p>For a quantitive study of the impact on cross-task generalization and single-task specialization, the authors fine-tune varying proportions of task-specific neurons. During fine-tuning, only the neurons specific to the current training task are trained. For measuring specialization performance, the test set of the training task (in-domain, ID) is used, while the test set of other tasks (out-of-domain, OOD) is used for measuring generalization performance.</p> <p>For qualitative analysis, the authors compute the task-specific neuron parameters cosine similarity within a model between the task used to train that model and test task, and study how this similarity varies across different layers of the model, aiming to investigate knowledge transfer between the test task and training task. Also, the authors compute the correlation coefficient between this parameter similarity and performance on corresponding test set, aiming to further demonstrate association between parameter similarity and generalization.</p> <h3 id="exploring-task-specific-neurons-to-mitigate-catastrophic-forgetting-in-lms">Exploring Task-Specific Neurons to Mitigate Catastrophic Forgetting in LMs</h3> <p>Because of parameter interference between tasks, an LM trained on multiple tasks can effectively handle multiple tasks but does not necessarily achieve optimal performance on a single task. Similarly, catastrophic forgetting can also be caused by parameter interference. The authors propose a Neuron-level Continuous learning Fine-Tuning (NCFT) method to mitigate catastrophic forgetting in continual learning.</p> <p>Given a sequence of tasks \(D_1, \dots, D_N\), the tasks arrive sequentially in the order of task sequence during the training stage. For current task \(D_n\), the authors update only the neuron-specific parameters of the current task, while keeping the other parameters frozen. During the test stage, the inference is executed as usual.</p> <h2 id="experiments-and-results">Experiments and Results</h2> <h3 id="1-do-task-specific-neurons-exist-in-lms-from-a-broad-perpspective">1. Do task-specific neurons exist in LMs, from a broad perpspective?</h3> <h4 id="experiment-1">Experiment-1</h4> <p>The authors deactivated task-specific neurons to conduct deactivation experiments. Deactivation was achieved by setting the activation value of these neurons to zero or by directly setting the corresponding parameter to zero. \(k\) was set to \(10\) in these experiments.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/table-1-480.webp 480w,/assets/img/towards-understanding-mtl/table-1-800.webp 800w,/assets/img/towards-understanding-mtl/table-1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/towards-understanding-mtl/table-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>As can be seen from Table-1, deactivating 10% task-specific neurons has a large negative impact on task-specific processing capacity whereas deactivating same number of randomly selected neurons results in a small impact.</p> <h4 id="experiment-2">Experiment-2</h4> <p>The authors conducted fine-tuning experiments where only task-specific neurons were updated with parameters and other neurons were frozen during training.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/table-2-480.webp 480w,/assets/img/towards-understanding-mtl/table-2-800.webp 800w,/assets/img/towards-understanding-mtl/table-2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/towards-understanding-mtl/table-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>As can be seen from Table-2, the fine-tuning approach to task-specific neurons yields remarkable improvements compared to the approach of fine-tuning randomly selected neurons. This is consistent across task categories (except Amazonfood - probably it has a good enough zero-shot performance).</p> <p>Based on Experiment-1 and Experiment-2, we can empirically assert the presence of task-specific neurons within LMs.</p> <h3 id="2-if-task-specific-neurons-exist-can-they-facilitate-the-understanding-of-the-multi-task-learning-mechanisms-in-lms">2. If task-specific neurons exist, can they facilitate the understanding of the multi-task learning mechanisms in LMs?</h3> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/figure-2-480.webp 480w,/assets/img/towards-understanding-mtl/figure-2-800.webp 800w,/assets/img/towards-understanding-mtl/figure-2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/towards-understanding-mtl/figure-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <h4 id="specialization-generalization-and-trade-off">Specialization, Generalization and trade-off</h4> <p>The authors controlled the proportion of fine-tuned task-specific neurons to conduct experiments on the various training-test combinations. Figure-2 shows results for all training-test combinations. In each subfigure, we only focus on the trend for each colored line. Comparisons between different color lines are meaningless because they represent different tasks.</p> <p>As the proportion of trained task-specific neurons increases, the specialization performance for both classification and generation tasks first ascends and then declines, reaching its peak at 70% for the classification task and at 50% for the generation task. This could be due to parameter interference between different tasks induced by simultaneous training of three tasks. This interference further results in specialization performance of a single task not exhibiting a continuous improvement as more parameters are trained.</p> <p>The authors also conducted ablation experiments where they trained a model for each task, meaning that finetuning of task-specific neurons was conducted individually. They observed a continous enhancement in performance as the proportion of neurons increases.</p> <p>As the proportion of trained task-specific neurons increases, the authors find a continuous increasing trend for the perfromance of generalization from the trained classification tasks to other classification tasks. Similar trend is also observed for generation tasks. The authors also look at overlap rate of task-specific neurons between training tasks and test tasks as:</p> \[overlap(x, y) = \frac{\mathcal{N}_x \cap \mathcal{N}_y}{\mathcal{N}_x \cup \mathcal{N}_y}\] <p>where \(\mathcal{N}_{tasks}\) denotes the set of task-specific neurons. Overall set of task-specific neurons of three training tasks is denoted as \(\mathcal{N}_x\) and the set of task-specific neurons of the test task is denoted as \(\mathcal{N}_y\). The authors found that as proportion of task-specific neurons increases, overlap rate also experiences a significant surge. A plausible explanation for this is that overlap of task-specific neurons contributes to transfer learning between tasks, ultimately resulting in consistently higher generalization performance.</p> <p>The authors also observed no generalization from classification tasks to generation tasks, which is probably because classification tasks are usually easier as they need to predict a single label.</p> <p>The authors found that when training all parameters of the model under the multi-task learning setup, inevitable interference among task occurs, thereby diminishing the efficacy of individual tasks to some degree. Furthermore, experiments illustrate the efficacy of controlling the appropriate proportion of fine-tuned task-specific neurons. Additionally there is a significant overlap of task-specific neurons and generalization performance across tasks. However, this overlap does not always guarantee deterministic generalization, as numerous factors also play pivotal roles.</p> <h4 id="parameters-of-task-specific-neurons">Parameters of Task-Specific Neurons</h4> <p>The authors evaluated similarity of specific neuron parameters for the training and test tasks aiming to conduct a qualitative analysis of generalization provenance. The authors trained a separate model for each of the six training tasks - \(M_1, \dots, M_6\). Then these models are tested on six out-of-domain test tasks - \(T_1, \dots, T_6\). In a particular layer, for model \(M_i\) and test task \(T_j\), \(\mathbf{P}_i^i\) and \(\mathbf{P}_j^i\) are used to denote the task-specific neuron parameters of training task \(i\) and test task \(j\) in \(M_i\) respectively. Cosine similarity between \(\mathbf{P}_i^i\) and \(\mathbf{P}_j^i\) is then computed. For test task \(T_j\), average of 6 similarities is calculated. Figure-3 illustrates the similarity of different layers for three different settings.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/figure-3-480.webp 480w,/assets/img/towards-understanding-mtl/figure-3-800.webp 800w,/assets/img/towards-understanding-mtl/figure-3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/towards-understanding-mtl/figure-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The authors findings suggest a correlation between generalization across different tasks and similarity of task-specific neuron parameters. When layers after a certain depth are reached, the model can learn shared knowledge between tasks, which contributes to generalization performance. The conclusions provide a guideline for improving generalization performance across tasks.</p> <h3 id="3-can-we-improve-lms-by-exploring-such-neurons">3. Can we improve LMs by exploring such neurons?</h3> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/table-3-480.webp 480w,/assets/img/towards-understanding-mtl/table-3-800.webp 800w,/assets/img/towards-understanding-mtl/table-3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/towards-understanding-mtl/table-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> <p>The authors conducted experiments to evaluate the effectiveness of the proposed NCFT method. The results are shown in Table-3. The proposed NCFT method significantly outperforms the baseline method, which is consistent with the authors’ hypothesis that the proposed method can mitigate catastrophic forgetting in continual learning.</p> <h2 id="summary">Summary</h2> <ol> <li>The authors presented a methodology framework for understanding multi-task learning and cross-task generalization of LLMs from the perspective of neurons.</li> <li>Using the framework, extensive analysis of LMs is conducted to identify task-specific neurons that are highly correlated with specific tasks.</li> <li>Using these task specific neurons, the authors investigated two common problems of LMs in multi-task learning and continuous learning: generalization and catastrophic forgetting.</li> <li>Authors found that the identified task-specific neurons is strongly associated with generalization.</li> <li>The parameter similarity of these neurons reflects degree of knowledge sharing, contributing to generalization.</li> <li>A neuron-level continuous fine-tuning method is proposed for effective mitigation of catastrophic forgetting in continual learning.</li> </ol> <h3 id="references">References</h3> <p>Leng, Y. and Xiong, D., 2024. Towards understanding multi-task learning (generalization) of llms via detecting and exploring task-specific neurons. arXiv preprint arXiv:2407.06488.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/2024-12-25-layer-by-layer.bib"></d-bibliography> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Hari Sri Venkata Naga Sai Kowndinya Renduchintala. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> </body> </html>