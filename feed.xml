<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kowndinya-renduchintala.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kowndinya-renduchintala.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-02T15:11:49+00:00</updated><id>https://kowndinya-renduchintala.github.io/feed.xml</id><title type="html">Kowndinya Renduchintala</title><subtitle>Kowndinya&apos;s Personal Website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models</title><link href="https://kowndinya-renduchintala.github.io/blog/2024/layer-by-layer/" rel="alternate" type="text/html" title="Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models"/><published>2024-12-24T00:00:00+00:00</published><updated>2024-12-24T00:00:00+00:00</updated><id>https://kowndinya-renduchintala.github.io/blog/2024/layer-by-layer</id><content type="html" xml:base="https://kowndinya-renduchintala.github.io/blog/2024/layer-by-layer/"><![CDATA[ <h2 id="introduction">Introduction</h2> <p>Instruction Tuning involves fine-tuning a language model on a collection of instruction-formatted multi-task datasets, with the goal of enabling the language model to generalize to unseen tasks. This paper investigates where and to what extent task-specific information is already encoded in the pre-trained language model and how instruction tuning affects the internal representations of these tasks in the model. Specifically, the authors investigate the following three research questions:</p> <h3 id="research-questions">Research Questions</h3> <ol> <li>To what extent are different NLP tasks already encoded in pre-trained LMs?</li> <li>How does instruction tuning modify the representational landscape of LMs?</li> <li>Do the representational effects of instruction tuning generalize to unseen tasks?</li> </ol> <h2 id="methodology">Methodology</h2> <p>Standard probing methods involve building a model to perform a downstream task based on intermediate representations, with the goal of quantifying encoded information in them. But these methods can be limited because they rely on different metrics to evaluate performance across various tasks, making it challenging to directly compare the amount of information stored about various tasks as diverse as sentiment analysis and translation.</p> <p>So, the authors use a sub-population technique called MOSSA - Model-Oriented Sub-population and Spectral Analysis - which involves comparing representations from two models - a <em>control</em> model and an <em>experimental</em> model. The control model is trained on the data relevant to the sub-population of interest (e.g., a specific task) and can be thought of as specialized in it. The experimental model is identical to the control model but is also trained on additional data from different sources (e.g., multiple tasks). MOSSA analyzes the differences in representations of the control and experimental models in order to understand what information is captured when a subset of the data is used versus the whole dataset. Intuitively, a high similarity between the experimental and control models indicates that the experimental model stores task-specific information learned by the control model, which was finetuned solely on the data from that task. For computing the similarity between representations, the authors use Central Kernel Alignment (CKA) metric, which measures alignment between representations in a kernel space, providing robust measure of similarity that is insensitive to scaling and centering.</p> <p>Formally, let \([T]\) be an index set of tasks, and let \(\mathbf{E}\) be the experimental model and \(\mathbf{C}_t\) be the control model for task \(t\in[T]\). Let us assume a set of inputs \(\mathcal{X}=\cup_{t=1}^{T}\mathcal{X}_t\), where each \(\mathcal{X}_t={x_{t,1}, \dots, x_{t,n}}\) represents a set of input instructions for task \(t\). <d-footnote>We assume that all sets have the same size n for simplicity.</d-footnote> For each \(t\in[T]\) and \(i\in[n]\), we apply the experimental model \(\mathbf{E}\) and the control model \(\mathbf{C}_t\) to the input instructions \(x_{t,i}\) to obtain two corresponding representations \(\mathbf{y}_{t,i}\in\mathbb{R}^{d_t}\) and \(\mathbf{z}_{t,i}\in\mathbb{R}^{d_t}\), respectively. Here, \(d\) is the dimension of the experimental model representations, and \(d_t\) is the dimension of the control model representations for task \(t\). To obtain the representations \(\mathbf{y}_{t,i}\) and \(\mathbf{z}_{t,i}\), the authors use the last token representation, as LMs are auto-regressive and the last token captures all input information. These representations can be extracted from any layers of the respective models. By stacking these vectors into two matrices for each task \(t\), the paired matrices \(\mathbf{Y}_t\in\mathbb{R}^{n\times d_t}\) and \(\mathbf{Z}_t\in\mathbb{R}^{n\times d_t}\) are obtained. The CKA between the representations of the experimental and control models for task \(t\) is then computed as follows:</p> <ul> <li>The kernel matrices \(K_{\mathbf{Y}_t}\in\mathbb{R}^{n\times n}\) and \(K_{\mathbf{Z}_t}\in\mathbb{R}^{n\times n}\) for the representations \(\mathbf{Y}_t\) and \(\mathbf{Z}_t\) are computed using the same kernel function (e.g., linear, Gaussian or polynomial)</li> <li>Kernel matrices are then centered by \(K_{\mathbf{Y}_t}=K_{\mathbf{Y}_t}-\frac{1}{n}\mathbf{1}K_{\mathbf{Y}_t}-\frac{1}{n}K_{\mathbf{Y}_t}\mathbf{1}+\frac{1}{n^2}\mathbf{1}K_{\mathbf{Y}_t}\mathbf{1}\) (similarly for \(K_{\mathbf{Z}_t}\) where \(\mathbf{1}\) is a matrix of ones)</li> <li>CKA is first computed as the Frobenius inner product of the centered Gram matrices: \(HSIC(K_{\mathbf{Y}_t}, K_{\mathbf{Z}_t})=Tr(K_{\mathbf{Y}_t}^TK_{\mathbf{Z}_t})\), where \(Tr\) denotes the trace of a matrix. CKA value is then normalized:</li> </ul> \[CKA(\mathbf{Y}_t, \mathbf{Z}_t)=\frac{HSIC(K_{\mathbf{Y}_t}, K_{\mathbf{Z}_t})}{\sqrt{HSIC(K_{\mathbf{Y}_t}, K_{\mathbf{Y}_t})HSIC(K_{\mathbf{Z}_t}, K_{\mathbf{Z}_t})}}\] <h2 id="experiments-and-results">Experiments and Results</h2> <p>60 NLP tasks from FLAN 2021 dataset are considered for the analysis. They are organized into 12 task clusters, where datasets within a given cluster belong to the same task type. To enhance instruction diversity, 10 unique natural language instruction templates are used for each dataset. The authors use the Llama-2-SFT (7B) model as a case study. There are two types of models: the experimental model \(\mathbf{E}\), finetuned using all \(T\) available tasks, and the single-task model \(\mathbf{C}_t\) for \(t\in[T]\), fine-tuned only on the \(t\)-th task. In some experiments, \(\mathbf{E}\) can also be the pre-trained model.</p> <h3 id="results">Results</h3> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/layer-by-layer/figure-2-480.webp 480w,/assets/img/layer-by-layer/figure-2-800.webp 800w,/assets/img/layer-by-layer/figure-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/layer-by-layer/figure-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="to-what-extent-are-different-nlp-tasks-already-encoded-in-pre-trained-lms">To what extent are different NLP tasks already encoded in pre-trained LMs?</h4> <p>Figure-2 shows the distribution of CKA similarities across all tasks and layers for the Llama-2 model. Llama-2 maintains high CKA similarities in earlier layers implying that representational changes in the earlier layers are minimal <em>across</em> tasks. However, in the middle and higher layers, there is a widespread variance in CKA <em>across</em> tasks i.e., some task representations in Llama-2 have high similarity with control model’s representations while others have low similarity. Since control models can be thought of as specialized in a particular task, this means that some tasks are better captured in Llama-2 model representations than others.</p> <p>To get a finer understanding, the authors analyzed CKA results at the task cluster level, where each cluster consists of a group of similar tasks.</p> <div class="single-image" style="max-width: 50%; width: auto; height: auto; margin: auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/layer-by-layer/figure-3-480.webp 480w,/assets/img/layer-by-layer/figure-3-800.webp 800w,/assets/img/layer-by-layer/figure-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/layer-by-layer/figure-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>For clusters like closed-book QA, commonsense reasoning, paraphrase detection, and sentiment analysis, which heavily rely on general linguistic and semantic understanding, the CKA similarity for Llama-2 is high, indicating that pre-trained LMs already encode these tasks well in their representations. Conversely, for clusters like coreference resolution, reading comprehension, structured data to text generation, summarization, and translation, which require specialized, structured or domain-specific knowledge involving complex transformations or extended context management, the CKA simiarities are low, suggesting that next token prediction at pre-training is insufficient for encoding these tasks.</p> <h4 id="how-does-instruction-tuning-modify-the-representational-landscape-of-lms">How does instruction tuning modify the representational landscape of LMs?</h4> <h5 id="mapping-layers-to-their-functionality">Mapping layers to their functionality</h5> <p>As illustrated in Figure-2, the CKA similarities between Llama-2-SFT and control models do not decrease as significantly as those for the pre-trained model across layers.</p> <ul> <li>In early layers (1 to 9), for many tasks, CKA scores are lower for Llama-2-SFT compared to Llama-2 i.e., the Llama-2-SFT representations diverge from those of control models which specialize in individual tasks. This means that training on multiple tasks encourages the model to learn more general representations in the lower layers. The authors call these layers as <em>shared layers</em>, because their representations are shared across tasks.</li> <li>In the middle layers (10-15), there is a signficant transition, with Llama-2-SFT model exhibiting high similarity to <em>all control models</em>. Since control models specialize in individual tasks, a high similarity means that these layers encode a high degree of task-specific information. The authors call these layers as <em>transitional layers</em>, as the transition to task-specific representations coccurs within these layers.</li> <li>This trend continues, albeit to a lesser extent, up to final layers (16-32), which the authors call <em>refinement layers</em>. In these layers, the model continues to refine its representations towards task-specific predictions.</li> </ul> <h5 id="examining-individual-task-clusters">Examining individual task clusters</h5> <p>As Figure-3 demonstrates, for tasks that are not well encoded in the pre-trained Llama-2 (structured data to text generation, translation), the CKA similarities for Llama-2-SFT remained high throughout layers 10-32 (transition and refinement layers). Instruction Tuning resulted in significant representational shifts, especially for these tasks.</p> <h4 id="do-the-representational-effects-of-instruction-tuning-generalize-to-unseen-tasks">Do the representational effects of instruction tuning generalize to unseen tasks?</h4> <p>To analyze how well the findings generalize to unseen tasks, the authors held out 7 out of 60 tasks from the FLAN 2021 dataset and instruction-tuned the model on remaining 53 tasks.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/layer-by-layer/figure-7-480.webp 480w,/assets/img/layer-by-layer/figure-7-800.webp 800w,/assets/img/layer-by-layer/figure-7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/layer-by-layer/figure-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li>For lower layers (upto 12), Llama-2 model exhibited slightly higher CKA similarities than Llama-2-SFT model for several tasks, indicating that while Llama-2-SFT model was not explicitly trained on these tasks, it produced more divergent representations in lower layers and thus more general than the ones produced by Llama-2.</li> <li>However, as we move to middle and higher layers, Llama-2-SFT model began matching and ultimately surpassing the CKA similarities of Llama-2 model.</li> </ul> <p>Also, high variances in CKA similarities across tasks, suggests that we cannot identify transition layers for Llama-2-SFT model in this setup, just shared and refinement layers.</p> <h3 id="conclusion">Conclusion</h3> <ul> <li>LMs instruction tuned on multiple tasks learned different representations in lower layers compared to LMs tuned on individual tasks. (Such representations could be shared and used across tasks)</li> <li>There are clear differences between pre-trained and instruction-tuned models, with most significant representational transformations occuring in the middle transitional layers. (further highlights the critical role of middle layers in encoding the specialized task knowledge induced by instruction tuning)</li> <li>In the refinement layers, instruction-tuned models continue to shape representations toward specific tasks but without substantial representational changes with respect to task-specific information.</li> <li>The mapping doesn’t generalize to unseen tasks, revealing that a potential additional reason for the strong generalization capabilities of instruction-tuned models to unseen tasks can be related to their multi-task nature of producing more general representations.</li> </ul> <h3 id="references">References</h3> <p>Zheng Zhao, Yftah Ziser, and Shay B Cohen. 2024. Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15195–15214, Miami, Florida, USA. Association for Computational Linguistics.</p>]]></content><author><name>H S V N S Kowndinya Renduchintala</name></author><category term="instruction-tuning"/><category term="interpretability"/><summary type="html"><![CDATA[&lt;!– &lt;h2 style="text-align: center;"&gt;TL;DR&lt;/h2&gt;]]></summary></entry><entry><title type="html">Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons</title><link href="https://kowndinya-renduchintala.github.io/blog/2024/towards-understanding-mtl/" rel="alternate" type="text/html" title="Towards Understanding Multi-Task Learning (Generalization) of LLMs via Detecting and Exploring Task-Specific Neurons"/><published>2024-12-24T00:00:00+00:00</published><updated>2024-12-24T00:00:00+00:00</updated><id>https://kowndinya-renduchintala.github.io/blog/2024/towards-understanding-mtl</id><content type="html" xml:base="https://kowndinya-renduchintala.github.io/blog/2024/towards-understanding-mtl/"><![CDATA[<h2 id="introduction">Introduction</h2> <p>Instruction Tuning involves fine-tuning a language model on a collection of instruction-formatted multi-task datasets, with the goal of enabling the language model to generalize to unseen tasks. This paper investigates the inner workings of multi-task learning (from the perspective of neurons) in LMs, which still remains an open question. Specifically, the authors investigate the following three research questions:</p> <h3 id="research-questions">Research Questions</h3> <ol> <li>Do tasks-specific neurons exist in LMs, from a broad perpspective?</li> <li>If they exist, can they facilitate the understanding of the multi-task learning mechanisms in LMs?</li> <li>Can we improve LMs by exploring such neurons?</li> </ol> <p>On a very high level, this paper (empirically) detects task-sensitive neurons in LMs via gradient attribution on task-specific data and derives insights into generalization across tasks with the detected task-specific neurons. Further, a <strong>N</strong>euron-level <strong>C</strong>ontinuous learning <strong>F</strong>ine-<strong>T</strong>uning (NCFT) method is proposed for mitigating catastrophic forgetting.</p> <h2 id="methodology">Methodology</h2> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/figure-1-480.webp 480w,/assets/img/towards-understanding-mtl/figure-1-800.webp 800w,/assets/img/towards-understanding-mtl/figure-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/towards-understanding-mtl/figure-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h3 id="identifying-task-specific-neurons">Identifying Task-Specific Neurons</h3> <p><strong>What is a neuron here?</strong> An autoregressive transformer-based LM (such as GPT-2) consists of an embedding layer, multiple residual blocks and an unembedding layer. Each residual block consists of a multi-head self-attention (MHA) module and a feed-forward network (FFN). The authors only focus on FFN citing that these have been demonstrated to store a large amount of parametric knowledge.</p> <p>The FFN module at layer \(i\) can be formulated as</p> \[\mathbf{h}^i=f(\mathbf{\tilde{h}^i}\mathbf{W}_1^i).\mathbf{W}_2^{i}\] <p>where \(\mathbf{\tilde{h}}^i\in\mathbb{R}^d\) denotes the output of the MHA module at layer \(i\), which is also the input of the current FFN module. \(\mathbf{h}^i\) denotes the output of the current FFN module. \(\mathbf{W}_1^i\in\mathbb{R}^{d\times d_{ff}}\) and \(\mathbf{W}_2^i\in\mathbb{R}^{d_{ff}\times d}\) are the weights of the FFN module at layer \(i\) and \(f(\cdot)\) denotes the activation function. A neuron is defined as a column in \(\mathbf{W}_1^i\) or \(\mathbf{W}_2^i\), which is a linear transformation of the input to the FFN module.</p> <p>Inspired from importance-based neuron fine-tuning studies and neuronal interpretability, the authors employ gradient-attribution to quantify each neuron’s relevance score for a given task. A relevance score \(\mathcal{R}_j^i\) is first defined of the \(j\)-th neuron in the \(i\)-th layer to a certain task:</p> \[\mathcal{R}_i^j=\left|\Delta \mathcal{L}(\mathbf{\omega}_j^i)\right|\] <p>where \(\mathbf{\omega}_j^i\) is the output of the \(j\)-th neuron in the \(i\)-th layer, and \(\Delta \mathcal{L}(\mathbf{\omega}_j^i)\) is the change in loss between setting \(\mathbf{\omega}_j^i\) to zero and the original value.</p> <p>Taylor Expansion can be used to approximate the change in loss when removing a particular neuron. Let \(\mathbf{\omega}^i\) be the output of the \(i\)-th layer and \(\Omega\) represent the set of other neurons. Assuming independence of each neuron in the model, the change of loss when removing the \(j\)-th neuron in layer \(i\) can be represented as:</p> \[\left| \Delta \mathcal{L}(\mathbf{\omega}_j^i) \right| = \left| \mathcal{L}(\Omega, \mathbf{\omega}_j^i=0) - \mathcal{L}(\Omega, \mathbf{\omega}_j^i) \right|\] <p>where \(\mathcal{L}(\Omega, \mathbf{\omega}_j^i=0)\) is the loss when the \(j\)-th neuron is pruned and \(\mathcal{L}(\Omega, \mathbf{\omega}_j^i)\) is loss if it is not pruned. The first-order Taylor series approximation for the function \(\mathcal{L}(\Omega, \mathbf{\omega}_j^i)\) at \(\mathbf{\omega}_j^i=0\) is:</p> \[\mathcal{L}(\Omega, \mathbf{\omega}_j^i) \approx \mathcal{L}(\Omega, \mathbf{\omega}_j^i=0) + \frac{\partial\mathcal{L}(\Omega, \mathbf{\omega}_j^i)}{\partial\mathbf{\omega}_j^i} \cdot \mathbf{\omega}_j^i\] <p>Hence, the relevance score can be approximated as:</p> \[\mathcal{R}_j^i \approx \left| \frac{\partial\mathcal{L}(\Omega, \mathbf{\omega}_j^i)}{\partial\mathbf{\omega}_j^i} \cdot \mathbf{\omega}_j^i \right|\] <p>Neurons with top \(k\%\) relevance scores are considered as task-specific neurons, where \(k\) is a predefined hyperparameter.</p> <h3 id="understanding-multi-task-learning-in-lms-by-analyzing-task-specific-neurons">Understanding Multi-Task Learning in LMs by analyzing task-specific neurons</h3> <p>For a quantitive study of the impact on cross-task generalization and single-task specialization, the authors fine-tune varying proportions of task-specific neurons. During fine-tuning, only the neurons specific to the current training task are trained. For measuring specialization performance, the test set of the training task (in-domain, ID) is used, while the test set of other tasks (out-of-domain, OOD) is used for measuring generalization performance.</p> <p>For qualitative analysis, the authors compute the task-specific neuron parameters cosine similarity within a model between the task used to train that model and test task, and study how this similarity varies across different layers of the model, aiming to investigate knowledge transfer between the test task and training task. Also, the authors compute the correlation coefficient between this parameter similarity and performance on corresponding test set, aiming to further demonstrate association between parameter similarity and generalization.</p> <h3 id="exploring-task-specific-neurons-to-mitigate-catastrophic-forgetting-in-lms">Exploring Task-Specific Neurons to Mitigate Catastrophic Forgetting in LMs</h3> <p>Because of parameter interference between tasks, an LM trained on multiple tasks can effectively handle multiple tasks but does not necessarily achieve optimal performance on a single task. Similarly, catastrophic forgetting can also be caused by parameter interference. The authors propose a Neuron-level Continuous learning Fine-Tuning (NCFT) method to mitigate catastrophic forgetting in continual learning.</p> <p>Given a sequence of tasks \(D_1, \dots, D_N\), the tasks arrive sequentially in the order of task sequence during the training stage. For current task \(D_n\), the authors update only the neuron-specific parameters of the current task, while keeping the other parameters frozen. During the test stage, the inference is executed as usual.</p> <h2 id="experiments-and-results">Experiments and Results</h2> <h3 id="1-do-task-specific-neurons-exist-in-lms-from-a-broad-perpspective">1. Do task-specific neurons exist in LMs, from a broad perpspective?</h3> <h4 id="experiment-1">Experiment-1</h4> <p>The authors deactivated task-specific neurons to conduct deactivation experiments. Deactivation was achieved by setting the activation value of these neurons to zero or by directly setting the corresponding parameter to zero. \(k\) was set to \(10\) in these experiments.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/table-1-480.webp 480w,/assets/img/towards-understanding-mtl/table-1-800.webp 800w,/assets/img/towards-understanding-mtl/table-1-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/towards-understanding-mtl/table-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>As can be seen from Table-1, deactivating 10% task-specific neurons has a large negative impact on task-specific processing capacity whereas deactivating same number of randomly selected neurons results in a small impact.</p> <h4 id="experiment-2">Experiment-2</h4> <p>The authors conducted fine-tuning experiments where only task-specific neurons were updated with parameters and other neurons were frozen during training.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/table-2-480.webp 480w,/assets/img/towards-understanding-mtl/table-2-800.webp 800w,/assets/img/towards-understanding-mtl/table-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/towards-understanding-mtl/table-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>As can be seen from Table-2, the fine-tuning approach to task-specific neurons yields remarkable improvements compared to the approach of fine-tuning randomly selected neurons. This is consistent across task categories (except Amazonfood - probably it has a good enough zero-shot performance).</p> <p>Based on Experiment-1 and Experiment-2, we can empirically assert the presence of task-specific neurons within LMs.</p> <h3 id="2-if-task-specific-neurons-exist-can-they-facilitate-the-understanding-of-the-multi-task-learning-mechanisms-in-lms">2. If task-specific neurons exist, can they facilitate the understanding of the multi-task learning mechanisms in LMs?</h3> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/figure-2-480.webp 480w,/assets/img/towards-understanding-mtl/figure-2-800.webp 800w,/assets/img/towards-understanding-mtl/figure-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/towards-understanding-mtl/figure-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="specialization-generalization-and-trade-off">Specialization, Generalization and trade-off</h4> <p>The authors controlled the proportion of fine-tuned task-specific neurons to conduct experiments on the various training-test combinations. Figure-2 shows results for all training-test combinations. In each subfigure, we only focus on the trend for each colored line. Comparisons between different color lines are meaningless because they represent different tasks.</p> <p>As the proportion of trained task-specific neurons increases, the specialization performance for both classification and generation tasks first ascends and then declines, reaching its peak at 70% for the classification task and at 50% for the generation task. This could be due to parameter interference between different tasks induced by simultaneous training of three tasks. This interference further results in specialization performance of a single task not exhibiting a continuous improvement as more parameters are trained.</p> <p>The authors also conducted ablation experiments where they trained a model for each task, meaning that finetuning of task-specific neurons was conducted individually. They observed a continous enhancement in performance as the proportion of neurons increases.</p> <p>As the proportion of trained task-specific neurons increases, the authors find a continuous increasing trend for the perfromance of generalization from the trained classification tasks to other classification tasks. Similar trend is also observed for generation tasks. The authors also look at overlap rate of task-specific neurons between training tasks and test tasks as:</p> \[overlap(x, y) = \frac{\mathcal{N}_x \cap \mathcal{N}_y}{\mathcal{N}_x \cup \mathcal{N}_y}\] <p>where \(\mathcal{N}_{tasks}\) denotes the set of task-specific neurons. Overall set of task-specific neurons of three training tasks is denoted as \(\mathcal{N}_x\) and the set of task-specific neurons of the test task is denoted as \(\mathcal{N}_y\). The authors found that as proportion of task-specific neurons increases, overlap rate also experiences a significant surge. A plausible explanation for this is that overlap of task-specific neurons contributes to transfer learning between tasks, ultimately resulting in consistently higher generalization performance.</p> <p>The authors also observed no generalization from classification tasks to generation tasks, which is probably because classification tasks are usually easier as they need to predict a single label.</p> <p>The authors found that when training all parameters of the model under the multi-task learning setup, inevitable interference among task occurs, thereby diminishing the efficacy of individual tasks to some degree. Furthermore, experiments illustrate the efficacy of controlling the appropriate proportion of fine-tuned task-specific neurons. Additionally there is a significant overlap of task-specific neurons and generalization performance across tasks. However, this overlap does not always guarantee deterministic generalization, as numerous factors also play pivotal roles.</p> <h4 id="parameters-of-task-specific-neurons">Parameters of Task-Specific Neurons</h4> <p>The authors evaluated similarity of specific neuron parameters for the training and test tasks aiming to conduct a qualitative analysis of generalization provenance. The authors trained a separate model for each of the six training tasks - \(M_1, \dots, M_6\). Then these models are tested on six out-of-domain test tasks - \(T_1, \dots, T_6\). In a particular layer, for model \(M_i\) and test task \(T_j\), \(\mathbf{P}_i^i\) and \(\mathbf{P}_j^i\) are used to denote the task-specific neuron parameters of training task \(i\) and test task \(j\) in \(M_i\) respectively. Cosine similarity between \(\mathbf{P}_i^i\) and \(\mathbf{P}_j^i\) is then computed. For test task \(T_j\), average of 6 similarities is calculated. Figure-3 illustrates the similarity of different layers for three different settings.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/figure-3-480.webp 480w,/assets/img/towards-understanding-mtl/figure-3-800.webp 800w,/assets/img/towards-understanding-mtl/figure-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/towards-understanding-mtl/figure-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The authors findings suggest a correlation between generalization across different tasks and similarity of task-specific neuron parameters. When layers after a certain depth are reached, the model can learn shared knowledge between tasks, which contributes to generalization performance. The conclusions provide a guideline for improving generalization performance across tasks.</p> <h3 id="3-can-we-improve-lms-by-exploring-such-neurons">3. Can we improve LMs by exploring such neurons?</h3> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/towards-understanding-mtl/table-3-480.webp 480w,/assets/img/towards-understanding-mtl/table-3-800.webp 800w,/assets/img/towards-understanding-mtl/table-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/towards-understanding-mtl/table-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>The authors conducted experiments to evaluate the effectiveness of the proposed NCFT method. The results are shown in Table-3. The proposed NCFT method significantly outperforms the baseline method, which is consistent with the authors’ hypothesis that the proposed method can mitigate catastrophic forgetting in continual learning.</p> <h2 id="summary">Summary</h2> <ol> <li>The authors presented a methodology framework for understanding multi-task learning and cross-task generalization of LLMs from the perspective of neurons.</li> <li>Using the framework, extensive analysis of LMs is conducted to identify task-specific neurons that are highly correlated with specific tasks.</li> <li>Using these task specific neurons, the authors investigated two common problems of LMs in multi-task learning and continuous learning: generalization and catastrophic forgetting.</li> <li>Authors found that the identified task-specific neurons is strongly associated with generalization.</li> <li>The parameter similarity of these neurons reflects degree of knowledge sharing, contributing to generalization.</li> <li>A neuron-level continuous fine-tuning method is proposed for effective mitigation of catastrophic forgetting in continual learning.</li> </ol> <h3 id="references">References</h3> <p>Leng, Y. and Xiong, D., 2024. Towards understanding multi-task learning (generalization) of llms via detecting and exploring task-specific neurons. arXiv preprint arXiv:2407.06488.</p>]]></content><author><name>H S V N S Kowndinya Renduchintala</name></author><category term="instruction-tuning"/><category term="interpretability"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">An Intro to Transformer-based Language Models (LMs)</title><link href="https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro/" rel="alternate" type="text/html" title="An Intro to Transformer-based Language Models (LMs)"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro</id><content type="html" xml:base="https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro/"><![CDATA[<p><strong>Note</strong>: This is just an article (not a paper) explaining the basics of transformer-based LMs.</p> <h2 id="what-is-a-transformer-based-lm">What is a Transformer-based LM?</h2> <p>Transformers are a class of artificial neural networks that are used to model sequences e.g., natural language text. In this article, we primarily focus on GPT-style transformers. At a fundamental level, given a piece of natural language text, these transformer-based LMs generate a probability distribution over the vocabulary. Upon repeatedly sampling from these distributions, new text can be generated. Hence the name <em>Generative</em> Pretrained Transformers(GPT).</p> <h2 id="how-are-transformers-pre-trained">How are transformers pre-trained?</h2> <p>Language Modeling is typically framed as a problem of <strong>unsupervised distribution estimation</strong> from a set of training examples \(\mathcal{T}=(x_1, x_2, \dots, x_N)\), where each \(x_i\) is composed of variable length sequences of tokens \((\mathbf{t_{i}^{(1)}}, \mathbf{t_{i}^{(2)}}, \dots ,\mathbf{t_{i}^{(N_i)}})\). Note that each \(\mathbf{t_i^{(k)}}\) is a \(\lvert\mathcal{V}\rvert\)-dimensional one-hot vector where \(\mathcal{V}\) is the vocabulary of the language being modeled.</p> <p>The joint probabilities over tokens can be written as the product of conditional probabilities as follows:</p> \[P(x_i)=\prod_{k=1}^{N_i}P\left(\mathbf{t_i^{(k)}} \mid\mathbf{t_i^{(1)}}, \mathbf{t_i^{(2)}}, \dots, \mathbf{t_i^{(k-1)}}\right)\] <p>Language Models (consisting of parameters \(\mathbf{\Theta}=\{\theta_i\}_{i=1}^{P}\) where \(P\) can be of the order of few billions), are trained to maximize the (log-)likelihood of text in their training set \(\mathcal{T}\) i.e., they try to estimate \(\mathbf{\Theta^*}\) where</p> \[\mathbf{\Theta^*}= \underset{\mathbf{\Theta}}{\mathrm{argmax}} \log \prod_{i=1}^{N}P(x_i;\mathbf{\Theta}) = \underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N}\log P(x_i;\mathbf{\Theta})\] \[=\underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N} \sum_{k=1}^{N_i} \log P\left(\mathbf{t_i^{(k)}} \mid\mathbf{t_i^{(1)}}, \mathbf{t_i^{(2)}}, \dots, \mathbf{t_i^{(k-1)}};\mathbf{\Theta}\right)\] \[\mathbf{\Theta^*}=\underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N} \sum_{k=1}^{N_i} \mathbf{t_i^{(k)}} . \log \mathbf{y_i^{(k)}}(\mathbf{t_i^{(1)}},\mathbf{t_i^{(2)}},\dots,\mathbf{t_i^{(k-1)}},\mathbf{\Theta})\] <p>Here, \(\mathbf{y_i^{(k)}}\left(\mathbf{t_i^{(1)}},\mathbf{t_i^{(2)}},\dots,\mathbf{t_i^{(k-1)}},\mathbf{\Theta}\right) \in \mathbb{R}^{\lvert\mathcal{V}\rvert}\) is the vector of probabilities that are predicted by the model for the token \(\mathbf{t_i^{(k)}}\), given previous tokens in the input and the model parameters \(\mathbf{\Theta}\). In the above equation, \(.\) represents inner product and logarithm is element-wise. The negative of the above objective is also called as Cross-Entropy Loss.</p> <p><strong>Remark</strong> Note that if you give a transformer \(N_i\) tokens, it predicts the next token for <strong><em>each</em></strong> of the \(N_i\) prefixes i.e., it produces \(N_i\) predictions. But why? Turns out it is easier to make one that does this. Moreover this also makes training more efficient, because you get \(N_i\) bits of feedback rather than just one.</p> <p><strong>Remark</strong> Also note that we make the transformer have <strong><em>causal attention</em></strong> (a.k.a. autoregressive) i.e., it can move information only forwards in the sequence. The prediction of what comes after \(K\) tokens is only a function of the first \(K\) tokens.</p> <p>The key takeaway is that transformers are <em>sequence modeling engines</em>. They do the same processing in parallel at each sequence position, can move information between positions using attention, and conceptually can take a sequence of arbitrary length (not actually true as we see later)</p> <p><strong>Inputs to a transformer</strong> The inputs to a transformer are a sequence of tokens. Each token is a \(\lvert\mathcal{V}\rvert\)-dimensional one-hot encoded vector.</p>]]></content><author><name>H S V N S Kowndinya Renduchintala</name></author><category term="transformers"/><category term="LMs"/><summary type="html"><![CDATA[Note: This is just an article (not a paper) explaining the basics of transformer-based LMs.]]></summary></entry></feed>