<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://kowndinya-renduchintala.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://kowndinya-renduchintala.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2024-12-27T01:05:24+00:00</updated><id>https://kowndinya-renduchintala.github.io/feed.xml</id><title type="html">Kowndinya Renduchintala</title><subtitle>Kowndinya&apos;s Personal Website. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models</title><link href="https://kowndinya-renduchintala.github.io/blog/2024/layer-by-layer/" rel="alternate" type="text/html" title="Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models"/><published>2024-12-24T00:00:00+00:00</published><updated>2024-12-24T00:00:00+00:00</updated><id>https://kowndinya-renduchintala.github.io/blog/2024/layer-by-layer</id><content type="html" xml:base="https://kowndinya-renduchintala.github.io/blog/2024/layer-by-layer/"><![CDATA[ <h2 id="introduction">Introduction</h2> <p>Instruction Tuning involves fine-tuning a language model on a collection of instruction-formatted multi-task datasets, with the goal of enabling the language model to generalize to unseen tasks. This paper investigates where and to what extent task-specific information is already encoded in the pre-trained language model and how instruction tuning affects the internal representations of these tasks in the model. Specifically, the authors investigate the following three research questions:</p> <h3 id="research-questions">Research Questions</h3> <ol> <li>To what extent are different NLP tasks already encoded in pre-trained LMs?</li> <li>How does instruction tuning modify the representational landscape of LMs?</li> <li>Do the representational effects of instruction tuning generalize to unseen tasks?</li> </ol> <h2 id="methodology">Methodology</h2> <p>Standard probing methods involve building a model to perform a downstream task based on intermediate representations, with the goal of quantifying encoded information in them. But these methods can be limited because they rely on different metrics to evaluate performance across various tasks, making it challenging to directly compare the amount of information stored about various tasks as diverse as sentiment analysis and translation.</p> <p>So, the authors use a sub-population technique called MOSSA - Model-Oriented Sub-population and Spectral Analysis - which involves comparing representations from two models - a <em>control</em> model and an <em>experimental</em> model. The control model is trained on the data relevant to the sub-population of interest (e.g., a specific task) and can be thought of as specialized in it. The experimental model is identical to the control model but is also trained on additional data from different sources (e.g., multiple tasks). MOSSA analyzes the differences in representations of the control and experimental models in order to understand what information is captured when a subset of the data is used versus the whole dataset. Intuitively, a high similarity between the experimental and control models indicates that the experimental model stores task-specific information learned by the control model, which was finetuned solely on the data from that task. For computing the similarity between representations, the authors use Central Kernel Alignment (CKA) metric, which measures alignment between representations in a kernel space, providing robust measure of similarity that is insensitive to scaling and centering.</p> <p>Formally, let \([T]\) be an index set of tasks, and let \(\mathbf{E}\) be the experimental model and \(\mathbf{C}_t\) be the control model for task \(t\in[T]\). Let us assume a set of inputs \(\mathcal{X}=\cup_{t=1}^{T}\mathcal{X}_t\), where each \(\mathcal{X}_t={x_{t,1}, \dots, x_{t,n}}\) represents a set of input instructions for task \(t\). <d-footnote>We assume that all sets have the same size n for simplicity.</d-footnote> For each \(t\in[T]\) and \(i\in[n]\), we apply the experimental model \(\mathbf{E}\) and the control model \(\mathbf{C}_t\) to the input instructions \(x_{t,i}\) to obtain two corresponding representations \(\mathbf{y}_{t,i}\in\mathbb{R}^{d_t}\) and \(\mathbf{z}_{t,i}\in\mathbb{R}^{d_t}\), respectively. Here, \(d\) is the dimension of the experimental model representations, and \(d_t\) is the dimension of the control model representations for task \(t\). To obtain the representations \(\mathbf{y}_{t,i}\) and \(\mathbf{z}_{t,i}\), the authors use the last token representation, as LMs are auto-regressive and the last token captures all input information. These representations can be extracted from any layers of the respective models. By stacking these vectors into two matrices for each task \(t\), the paired matrices \(\mathbf{Y}_t\in\mathbb{R}^{n\times d_t}\) and \(\mathbf{Z}_t\in\mathbb{R}^{n\times d_t}\) are obtained. The CKA between the representations of the experimental and control models for task \(t\) is then computed as follows:</p> <ul> <li>The kernel matrices \(K_{\mathbf{Y}_t}\in\mathbb{R}^{n\times n}\) and \(K_{\mathbf{Z}_t}\in\mathbb{R}^{n\times n}\) for the representations \(\mathbf{Y}_t\) and \(\mathbf{Z}_t\) are computed using the same kernel function (e.g., linear, Gaussian or polynomial)</li> <li>Kernel matrices are then centered by \(K_{\mathbf{Y}_t}=K_{\mathbf{Y}_t}-\frac{1}{n}\mathbf{1}K_{\mathbf{Y}_t}-\frac{1}{n}K_{\mathbf{Y}_t}\mathbf{1}+\frac{1}{n^2}\mathbf{1}K_{\mathbf{Y}_t}\mathbf{1}\) (similarly for \(K_{\mathbf{Z}_t}\) where \(\mathbf{1}\) is a matrix of ones)</li> <li>CKA is first computed as the Frobenius inner product of the centered Gram matrices: \(HSIC(K_{\mathbf{Y}_t}, K_{\mathbf{Z}_t})=Tr(K_{\mathbf{Y}_t}^TK_{\mathbf{Z}_t})\), where \(Tr\) denotes the trace of a matrix. CKA value is then normalized:</li> </ul> \[CKA(\mathbf{Y}_t, \mathbf{Z}_t)=\frac{HSIC(K_{\mathbf{Y}_t}, K_{\mathbf{Z}_t})}{\sqrt{HSIC(K_{\mathbf{Y}_t}, K_{\mathbf{Y}_t})HSIC(K_{\mathbf{Z}_t}, K_{\mathbf{Z}_t})}}\] <h2 id="experiments-and-results">Experiments and Results</h2> <p>60 NLP tasks from FLAN 2021 dataset are considered for the analysis. They are organized into 12 task clusters, where datasets within a given cluster belong to the same task type. To enhance instruction diversity, 10 unique natural language instruction templates are used for each dataset. The authors use the Llama-2-SFT (7B) model as a case study. There are two types of models: the experimental model \(\mathbf{E}\), finetuned using all \(T\) available tasks, and the single-task model \(\mathbf{C}_t\) for \(t\in[T]\), fine-tuned only on the \(t\)-th task. In some experiments, \(\mathbf{E}\) can also be the pre-trained model.</p> <h3 id="results">Results</h3> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/layer-by-layer/figure-2-480.webp 480w,/assets/img/layer-by-layer/figure-2-800.webp 800w,/assets/img/layer-by-layer/figure-2-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/layer-by-layer/figure-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <h4 id="to-what-extent-are-different-nlp-tasks-already-encoded-in-pre-trained-lms">To what extent are different NLP tasks already encoded in pre-trained LMs?</h4> <p>Figure-2 shows the distribution of CKA similarities across all tasks and layers for the Llama-2 model. Llama-2 maintains high CKA similarities in earlier layers implying that representational changes in the earlier layers are minimal <em>across</em> tasks. However, in the middle and higher layers, there is a widespread variance in CKA <em>across</em> tasks i.e., some task representations in Llama-2 have high similarity with control modelâ€™s representations while others have low similarity. Since control models can be thought of as specialized in a particular task, this means that some tasks are better captured in Llama-2 model representations than others.</p> <p>To get a finer understanding, the authors analyzed CKA results at the task cluster level, where each cluster consists of a group of similar tasks.</p> <div class="single-image" style="max-width: 50%; width: auto; height: auto; margin: auto;"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/layer-by-layer/figure-3-480.webp 480w,/assets/img/layer-by-layer/figure-3-800.webp 800w,/assets/img/layer-by-layer/figure-3-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/layer-by-layer/figure-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <p>For clusters like closed-book QA, commonsense reasoning, paraphrase detection, and sentiment analysis, which heavily rely on general linguistic and semantic understanding, the CKA similarity for Llama-2 is high, indicating that pre-trained LMs already encode these tasks well in their representations. Conversely, for clusters like coreference resolution, reading comprehension, structured data to text generation, summarization, and translation, which require specialized, structured or domain-specific knowledge involving complex transformations or extended context management, the CKA simiarities are low, suggesting that next token prediction at pre-training is insufficient for encoding these tasks.</p> <h4 id="how-does-instruction-tuning-modify-the-representational-landscape-of-lms">How does instruction tuning modify the representational landscape of LMs?</h4> <h5 id="mapping-layers-to-their-functionality">Mapping layers to their functionality</h5> <p>As illustrated in Figure-2, the CKA similarities between Llama-2-SFT and control models do not decrease as significantly as those for the pre-trained model across layers.</p> <ul> <li>In early layers (1 to 9), for many tasks, CKA scores are lower for Llama-2-SFT compared to Llama-2 i.e., the Llama-2-SFT representations diverge from those of control models which specialize in individual tasks. This means that training on multiple tasks encourages the model to learn more general representations in the lower layers. The authors call these layers as <em>shared layers</em>, because their representations are shared across tasks.</li> <li>In the middle layers (10-15), there is a signficant transition, with Llama-2-SFT model exhibiting high similarity to <em>all control models</em>. Since control models specialize in individual tasks, a high similarity means that these layers encode a high degree of task-specific information. The authors call these layers as <em>transitional layers</em>, as the transition to task-specific representations coccurs within these layers.</li> <li>This trend continues, albeit to a lesser extent, up to final layers (16-32), which the authors call <em>refinement layers</em>. In these layers, the model continues to refine its representations towards task-specific predictions.</li> </ul> <h5 id="examining-individual-task-clusters">Examining individual task clusters</h5> <p>As Figure-3 demonstrates, for tasks that are not well encoded in the pre-trained Llama-2 (structured data to text generation, translation), the CKA similarities for Llama-2-SFT remained high throughout layers 10-32 (transition and refinement layers). Instruction Tuning resulted in significant representational shifts, especially for these tasks.</p> <h4 id="do-the-representational-effects-of-instruction-tuning-generalize-to-unseen-tasks">Do the representational effects of instruction tuning generalize to unseen tasks?</h4> <p>To analyze how well the findings generalize to unseen tasks, the authors held out 7 out of 60 tasks from the FLAN 2021 dataset and instruction-tuned the model on remaining 53 tasks.</p> <div class="single-image"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/layer-by-layer/figure-7-480.webp 480w,/assets/img/layer-by-layer/figure-7-800.webp 800w,/assets/img/layer-by-layer/figure-7-1400.webp 1400w," sizes="95vw" type="image/webp"/> <img src="/assets/img/layer-by-layer/figure-7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <ul> <li>For lower layers (upto 12), Llama-2 model exhibited slightly higher CKA similarities than Llama-2-SFT model for several tasks, indicating that while Llama-2-SFT model was not explicitly trained on these tasks, it produced more divergent representations in lower layers and thus more general than the ones produced by Llama-2.</li> <li>However, as we move to middle and higher layers, Llama-2-SFT model began matching and ultimately surpassing the CKA similarities of Llama-2 model.</li> </ul> <p>Also, high variances in CKA similarities across tasks, suggests that we cannot identify transition layers for Llama-2-SFT model in this setup, just shared and refinement layers.</p> <h3 id="conclusion">Conclusion</h3> <ul> <li>LMs instruction tuned on multiple tasks learned different representations in lower layers compared to LMs tuned on individual tasks. (Such representations could be shared and used across tasks)</li> <li>There are clear differences between pre-trained and instruction-tuned models, with most significant representational transformations occuring in the middle transitional layers. (further highlights the critical role of middle layers in encoding the specialized task knowledge induced by instruction tuning)</li> <li>In the refinement layers, instruction-tuned models continue to shape representations toward specific tasks but without substantial representational changes with respect to task-specific information.</li> <li>The mapping doesnâ€™t generalize to unseen tasks, revealing that a potential additional reason for the strong generalization capabilities of instruction-tuned models to unseen tasks can be related to their multi-task nature of producing more general representations.</li> </ul> <h3 id="references">References</h3> <p>Zheng Zhao, Yftah Ziser, and Shay B Cohen. 2024. Layer by Layer: Uncovering Where Multi-Task Learning Happens in Instruction-Tuned Large Language Models. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 15195â€“15214, Miami, Florida, USA. Association for Computational Linguistics.</p>]]></content><author><name>H S V N S Kowndinya Renduchintala</name></author><category term="instruction-tuning"/><category term="interpretability"/><summary type="html"><![CDATA[&lt;!â€“ &lt;h2 style="text-align: center;"&gt;TL;DR&lt;/h2&gt;]]></summary></entry><entry><title type="html">An Intro to Transformer-based Language Models (LMs)</title><link href="https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro/" rel="alternate" type="text/html" title="An Intro to Transformer-based Language Models (LMs)"/><published>2024-12-15T00:00:00+00:00</published><updated>2024-12-15T00:00:00+00:00</updated><id>https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro</id><content type="html" xml:base="https://kowndinya-renduchintala.github.io/blog/2024/transformer-intro/"><![CDATA[<p><strong>Note</strong>: This is just an article (not a paper) explaining the basics of transformer-based LMs.</p> <h2 id="what-is-a-transformer-based-lm">What is a Transformer-based LM?</h2> <p>Transformers are a class of artificial neural networks that are used to model sequences e.g., natural language text. In this article, we primarily focus on GPT-style transformers. At a fundamental level, given a piece of natural language text, these transformer-based LMs generate a probability distribution over the vocabulary. Upon repeatedly sampling from these distributions, new text can be generated. Hence the name <em>Generative</em> Pretrained Transformers(GPT).</p> <h2 id="how-are-transformers-pre-trained">How are transformers pre-trained?</h2> <p>Language Modeling is typically framed as a problem of <strong>unsupervised distribution estimation</strong> from a set of training examples \(\mathcal{T}=(x_1, x_2, \dots, x_N)\), where each \(x_i\) is composed of variable length sequences of tokens \((\mathbf{t_{i}^{(1)}}, \mathbf{t_{i}^{(2)}}, \dots ,\mathbf{t_{i}^{(N_i)}})\). Note that each \(\mathbf{t_i^{(k)}}\) is a \(\lvert\mathcal{V}\rvert\)-dimensional one-hot vector where \(\mathcal{V}\) is the vocabulary of the language being modeled.</p> <p>The joint probabilities over tokens can be written as the product of conditional probabilities as follows:</p> \[P(x_i)=\prod_{k=1}^{N_i}P\left(\mathbf{t_i^{(k)}} \mid\mathbf{t_i^{(1)}}, \mathbf{t_i^{(2)}}, \dots, \mathbf{t_i^{(k-1)}}\right)\] <p>Language Models (consisting of parameters \(\mathbf{\Theta}=\{\theta_i\}_{i=1}^{P}\) where \(P\) can be of the order of few billions), are trained to maximize the (log-)likelihood of text in their training set \(\mathcal{T}\) i.e., they try to estimate \(\mathbf{\Theta^*}\) where</p> \[\mathbf{\Theta^*}= \underset{\mathbf{\Theta}}{\mathrm{argmax}} \log \prod_{i=1}^{N}P(x_i;\mathbf{\Theta}) = \underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N}\log P(x_i;\mathbf{\Theta})\] \[=\underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N} \sum_{k=1}^{N_i} \log P\left(\mathbf{t_i^{(k)}} \mid\mathbf{t_i^{(1)}}, \mathbf{t_i^{(2)}}, \dots, \mathbf{t_i^{(k-1)}};\mathbf{\Theta}\right)\] \[\mathbf{\Theta^*}=\underset{\mathbf{\Theta}}{\mathrm{argmax}} \sum_{i=1}^{N} \sum_{k=1}^{N_i} \mathbf{t_i^{(k)}} . \log \mathbf{y_i^{(k)}}(\mathbf{t_i^{(1)}},\mathbf{t_i^{(2)}},\dots,\mathbf{t_i^{(k-1)}},\mathbf{\Theta})\] <p>Here, \(\mathbf{y_i^{(k)}}\left(\mathbf{t_i^{(1)}},\mathbf{t_i^{(2)}},\dots,\mathbf{t_i^{(k-1)}},\mathbf{\Theta}\right) \in \mathbb{R}^{\lvert\mathcal{V}\rvert}\) is the vector of probabilities that are predicted by the model for the token \(\mathbf{t_i^{(k)}}\), given previous tokens in the input and the model parameters \(\mathbf{\Theta}\). In the above equation, \(.\) represents inner product and logarithm is element-wise. The negative of the above objective is also called as Cross-Entropy Loss.</p> <p><strong>Remark</strong> Note that if you give a transformer \(N_i\) tokens, it predicts the next token for <strong><em>each</em></strong> of the \(N_i\) prefixes i.e., it produces \(N_i\) predictions. But why? Turns out it is easier to make one that does this. Moreover this also makes training more efficient, because you get \(N_i\) bits of feedback rather than just one.</p> <p><strong>Remark</strong> Also note that we make the transformer have <strong><em>causal attention</em></strong> (a.k.a. autoregressive) i.e., it can move information only forwards in the sequence. The prediction of what comes after \(K\) tokens is only a function of the first \(K\) tokens.</p> <p>The key takeaway is that transformers are <em>sequence modeling engines</em>. They do the same processing in parallel at each sequence position, can move information between positions using attention, and conceptually can take a sequence of arbitrary length (not actually true as we see later)</p> <p><strong>Inputs to a transformer</strong> The inputs to a transformer are a sequence of tokens. Each token is a \(\lvert\mathcal{V}\rvert\)-dimensional one-hot encoded vector.</p>]]></content><author><name>H S V N S Kowndinya Renduchintala</name></author><category term="transformers"/><category term="LMs"/><summary type="html"><![CDATA[Note: This is just an article (not a paper) explaining the basics of transformer-based LMs.]]></summary></entry></feed>